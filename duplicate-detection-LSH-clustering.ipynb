{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e27c771-2b34-4e84-a780-3f019ce19e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages Imported\n"
     ]
    }
   ],
   "source": [
    "# Packages\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "from itertools import combinations\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "print(\"Packages Imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e37159a0-49ac-4e72-83dd-fb696e487029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# 1: [FUNCTIONS] Extract model words and create binary product vectors\n",
    "# ======================================================================\n",
    "\n",
    "def load_data():\n",
    "    with open(\"TVs-all-merged.json\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    modelIDs = []\n",
    "    shops = []\n",
    "    titles = []\n",
    "    features = []\n",
    "\n",
    "    for key, value_list in data.items():\n",
    "        for record in value_list:\n",
    "            modelIDs.append(record.get(\"modelID\", None))\n",
    "            shops.append(record.get(\"shop\", None))\n",
    "            titles.append(record.get(\"title\", None))\n",
    "            features.append(record.get(\"featuresMap\", None))\n",
    "        \n",
    "    # Combine into a DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"modelId\": modelIDs,\n",
    "        \"shop\": shops,\n",
    "        \"title\": titles,\n",
    "        \"features\": features\n",
    "    })\n",
    "\n",
    "    df['uniqueId'] = df.index\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_brand(title):\n",
    "    # List of known TV brands\n",
    "    known_brands = [\"apple\", \"affinity\", \"avue\", \"azend\", \"coby\", \"contex\", \"compaq\", \n",
    "    \"craig\", \"curtisyoung\", \"elite\", \"elo\", \"epson\", \"gpx\", \"haier\", \n",
    "    \"hannspree\", \"hello kitty\", \"hp\", \"dynex\", \"hisense\", \"hiteker\", \n",
    "    \"insignia\", \"jvc\", \"lg\", \"magnavox\", \"mitsubishi\", \"naxa\", \"nec\", \n",
    "    \"optoma\", \"panasonic\", \"philips\", \"proscan\", \"sceptre\", \"pyle\", \n",
    "    \"rca\", \"sanyo\", \"sansui\", \"seiki\", \"sony\", \"samsung\", \"sigmac\", \n",
    "    \"sharp\", \"sunbritetv\", \"supersonic\", \"tcl\", \"toshiba\", \"upstar\", \n",
    "    \"venturer\", \"vizio\", \"viewsonic\", \"viore\", \"westinghouse\"\n",
    "    ]\n",
    "    # Check if product is from known brand\n",
    "    title_lower = title.lower()\n",
    "    for brand in known_brands:\n",
    "        if brand in title_lower:\n",
    "            return brand\n",
    "    return None\n",
    "\n",
    "def clean_title(title):\n",
    "    \n",
    "    title = title.replace(\"'\", \"' \").replace('\"', '\" ')\n",
    "    \n",
    "    inch_variants = ['inch', 'inches', '\"', '-inch', ' inch', 'inch ', 'inch', '-inch', '”', \"'\"]\n",
    "    for variant in inch_variants:\n",
    "        title = title.replace(variant, \"inch \")\n",
    "\n",
    "    hz_variants = ['hertz', 'hz', ' hz', '-hz', 'hz']\n",
    "    for variant in hz_variants:\n",
    "        title = title.replace(variant, \"hz \")\n",
    "    \n",
    "    title = title.replace(\"/\", \"\").replace(\"-\", \"\")\n",
    "    \n",
    "    title = title.lower()\n",
    "    \n",
    "    return title\n",
    "\n",
    "\n",
    "def extract_model_words(title):\n",
    "    pattern = r'\\b[a-zA-Z0-9]+\\b' \n",
    "    matches = re.findall(pattern, title)\n",
    "    \n",
    "    model_words = [word for word in matches if any(char.isdigit() for char in word) and any(char.isalpha() for char in word)]\n",
    "    \n",
    "    unique_model_words = list(dict.fromkeys(model_words))\n",
    "    \n",
    "    return unique_model_words\n",
    "\n",
    "def create_binary_vectors(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Extract model words for each product\n",
    "    df['model words'] = df['title'].apply(lambda t: extract_model_words(clean_title(t)))\n",
    "\n",
    "    # Extract all model words from all product titles\n",
    "    all_model_words = set()\n",
    "    for model_words in df['model words']:\n",
    "        all_model_words.update(model_words)\n",
    "\n",
    "    model_word_list = sorted(all_model_words)\n",
    "\n",
    "    # Use product_id as the product identifier\n",
    "    uniqueIds = df['uniqueId'].tolist()\n",
    "\n",
    "    # Construct the binary matrix\n",
    "    binary_matrix = []\n",
    "    for mw in model_word_list:\n",
    "        binary_row = [1 if mw in model_words else 0 for model_words in df['model words']]\n",
    "        binary_matrix.append(binary_row)\n",
    "\n",
    "    binary_df = pd.DataFrame(binary_matrix, columns=uniqueIds, index=model_word_list)\n",
    "    return binary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bea30f7-33fa-46fc-a539-7d62bc1c8e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# 2: [Functions] Create Signature Matrix with Min-Hashing\n",
    "# =============================================================\n",
    "\n",
    "def min_hashing(binary_matrix, n_hashes):\n",
    "    \"\"\"\n",
    "    Performs min-hashing on the binary matrix with progress bars.\n",
    "    \n",
    "    Parameters:\n",
    "        binary_matrix (pd.DataFrame): Binary product vectors (model words as rows, products as columns).\n",
    "        p (int): Number of min-hash functions (permutations).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Signature matrix with p rows (hash functions) and columns as products.\n",
    "    \"\"\"\n",
    "    # Set seed for reproducibility\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    n_rows, n_columns = binary_matrix.shape\n",
    "    signature_matrix = np.full((n_hashes, n_columns), np.inf)\n",
    "    \n",
    "    # Generate random permutations of row indices\n",
    "    row_indices = np.arange(n_rows)\n",
    "    \n",
    "    for i in range(n_hashes): \n",
    "        np.random.shuffle(row_indices)  \n",
    "        for col_idx in range(n_columns):\n",
    "            for perm_row_idx in row_indices:\n",
    "                if binary_matrix.iloc[perm_row_idx, col_idx] == 1:  \n",
    "                    signature_matrix[i, col_idx] = perm_row_idx  \n",
    "                    break\n",
    "    \n",
    "    # Convert signature matrix to DataFrame \n",
    "    signature_df = pd.DataFrame(signature_matrix, columns=binary_matrix.columns)\n",
    "    return signature_df\n",
    "\n",
    "def verify_signature_matrix(binary_matrix, signature_matrix):\n",
    "    \"\"\"\n",
    "    Verifies that the signature matrix approximates Jaccard Similarity.\n",
    "\n",
    "    Parameters:\n",
    "        binary_matrix (pd.DataFrame): Original binary matrix.\n",
    "        signature_matrix (pd.DataFrame): Generated signature matrix.\n",
    "\n",
    "    Returns:\n",
    "        dict: Summary statistics including MAE, MSE, and correlation.\n",
    "    \"\"\"\n",
    "    exact_similarities = []\n",
    "    approx_similarities = []\n",
    "    \n",
    "    # Compute exact and approximate Jaccard similarities\n",
    "    for i in range(signature_matrix.shape[1]):\n",
    "        for j in range(i + 1, signature_matrix.shape[1]):\n",
    "            vector1 = binary_matrix.iloc[:, i].values\n",
    "            vector2 = binary_matrix.iloc[:, j].values\n",
    "            \n",
    "            # Exact Jaccard similarity\n",
    "            jaccard_exact = np.sum(np.logical_and(vector1, vector2)) / np.sum(np.logical_or(vector1, vector2))\n",
    "            exact_similarities.append(jaccard_exact)\n",
    "            \n",
    "            # Approximate Jaccard similarity\n",
    "            signature1 = signature_matrix.iloc[:, i].values\n",
    "            signature2 = signature_matrix.iloc[:, j].values\n",
    "            jaccard_approx = np.sum(signature1 == signature2) / signature_matrix.shape[0]\n",
    "            approx_similarities.append(jaccard_approx)\n",
    "    \n",
    "    # Convert to numpy arrays for calculations\n",
    "    exact_similarities = np.array(exact_similarities)\n",
    "    approx_similarities = np.array(approx_similarities)\n",
    "    \n",
    "    # Calculate summary metrics\n",
    "    mae = np.mean(np.abs(exact_similarities - approx_similarities))\n",
    "    mse = np.mean((exact_similarities - approx_similarities) ** 2)\n",
    "    correlation = np.corrcoef(exact_similarities, approx_similarities)[0, 1]\n",
    "    \n",
    "    return {\n",
    "        \"Mean Absolute Error (MAE)\": mae,\n",
    "        \"Mean Squared Error (MSE)\": mse,\n",
    "        \"Correlation\": correlation,\n",
    "        \"Exact Similarities Mean\": np.mean(exact_similarities),\n",
    "        \"Approx Similarities Mean\": np.mean(approx_similarities)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf3b3d6d-a72a-4060-9565-f3a21d624ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# 3: [Functions] Apply LSH to find Nearest Neighbours\n",
    "# =============================================================\n",
    "def pad_signature_matrix(signature_matrix, target_rows=None):\n",
    "    \"\"\"\n",
    "    Pads the signature matrix to ensure the number of rows is divisible by common values of r.\n",
    "\n",
    "    Parameters:\n",
    "        signature_matrix (pd.DataFrame): Original signature matrix.\n",
    "        target_rows (int, optional): Desired total number of rows. If None, calculate the next multiple of 10.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Padded signature matrix.\n",
    "    \"\"\"\n",
    "    current_rows, current_columns = signature_matrix.shape\n",
    "\n",
    "    if target_rows is None:\n",
    "        # Find the next multiple of 10 greater than the current number of rows\n",
    "        target_rows = (current_rows + 9) // 10 * 10  # Round up to the nearest multiple of 10\n",
    "\n",
    "    if target_rows < current_rows:\n",
    "        raise ValueError(\"Target rows cannot be smaller than the current number of rows.\")\n",
    "\n",
    "    # Calculate the number of rows to add\n",
    "    padding_rows = target_rows - current_rows\n",
    "    if padding_rows > 0:\n",
    "        # Create a DataFrame of padding rows with np.inf\n",
    "        padding = pd.DataFrame(np.inf, index=range(padding_rows), columns=signature_matrix.columns)\n",
    "        signature_matrix = pd.concat([signature_matrix, padding], ignore_index=True)\n",
    "\n",
    "    print(f\"Padded signature matrix to {target_rows} rows.\")\n",
    "    return signature_matrix\n",
    "\n",
    "def find_b_r_from_t(n, t, max_padding_allowed=15):\n",
    "    \"\"\"\n",
    "    Given n, t, and a maximum allowed padding in terms of rows,\n",
    "    find (b, r) that yields (1/b)^(1/r) closest to t, with b*r - n <= max_padding_allowed.\n",
    "    \"\"\"\n",
    "    best_diff = float('inf')\n",
    "    best_b = None\n",
    "    best_r = None\n",
    "    best_padding = None\n",
    "\n",
    "    for r in range(1, n+1):\n",
    "        b = math.ceil(n / r)\n",
    "        b_r = b * r\n",
    "        padding = b_r - n\n",
    "        if padding <= max_padding_allowed:\n",
    "            t_prime = (1/b)**(1/r)\n",
    "            diff = abs(t_prime - t)\n",
    "            if diff < best_diff or (diff == best_diff and (best_padding is None or padding < best_padding)):\n",
    "                best_diff = diff\n",
    "                best_b = b\n",
    "                best_r = r\n",
    "                best_padding = padding\n",
    "\n",
    "    # If still None, no pair met the constraint, so choose best ignoring it\n",
    "    if best_b is None:\n",
    "        best_diff = float('inf')\n",
    "        for r in range(1, n+1):\n",
    "            b = math.ceil(n / r)\n",
    "            t_prime = (1/b)**(1/r)\n",
    "            diff = abs(t_prime - t)\n",
    "            if diff < best_diff:\n",
    "                best_diff = diff\n",
    "                best_b = b\n",
    "                best_r = r\n",
    "\n",
    "    return best_b, best_r, best_diff\n",
    "    \n",
    "\n",
    "def apply_lsh(signature_matrix, t):\n",
    "    \"\"\"\n",
    "    Apply LSH given a desired threshold t.\n",
    "    This function:\n",
    "    1. Determines n from the signature_matrix.\n",
    "    2. Finds suitable (b, r) from t.\n",
    "    3. Applies LSH with those parameters.\n",
    "    \n",
    "    Returns:\n",
    "    - candidate_pairs: set of (product1, product2)\n",
    "    - b, r: chosen parameters\n",
    "    - t_actual: the t' achieved by the chosen pair (1/b)^(1/r)\n",
    "    \"\"\"\n",
    "    # Convert signature_matrix to numpy if needed\n",
    "    if isinstance(signature_matrix, np.ndarray):\n",
    "        pass\n",
    "    else:\n",
    "        signature_matrix = signature_matrix.values\n",
    "\n",
    "    n, N = signature_matrix.shape\n",
    "    b, r, _ = find_b_r_from_t(n, t)\n",
    "    # Now we have b and r. Compute t' for reference\n",
    "    t_actual = (1/b)**(1/r)\n",
    "\n",
    "    # Pad the signature matrix if necessary\n",
    "    new_n = b * r\n",
    "    if new_n > n:\n",
    "        pad_rows = new_n - n\n",
    "        # Use a stable padding value, e.g., -1\n",
    "        pad_matrix = np.full((pad_rows, N), -1)\n",
    "        signature_matrix = np.vstack([signature_matrix, pad_matrix])\n",
    "        n = new_n\n",
    "\n",
    "    candidate_pairs = set()\n",
    "    start = 0\n",
    "    for band_idx in range(b):\n",
    "        end = start + r\n",
    "        band = signature_matrix[start:end, :]\n",
    "\n",
    "        buckets = {}\n",
    "        for col_idx in range(N):\n",
    "            band_slice = tuple(band[:, col_idx])\n",
    "            if band_slice not in buckets:\n",
    "                buckets[band_slice] = []\n",
    "            buckets[band_slice].append(col_idx)\n",
    "\n",
    "        # Extract candidate pairs from buckets\n",
    "        for bucket_items in buckets.values():\n",
    "            if len(bucket_items) > 1:\n",
    "                for i in range(len(bucket_items)):\n",
    "                    for j in range(i+1, len(bucket_items)):\n",
    "                        p1 = bucket_items[i]\n",
    "                        p2 = bucket_items[j]\n",
    "                        candidate_pairs.add((min(p1,p2), max(p1,p2)))\n",
    "\n",
    "        start = end\n",
    "\n",
    "    return candidate_pairs, b, r, t_actual\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16c002a4-2927-4b7d-9881-5a362fe7cf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================================================\n",
    "# 4: [Functions] Apply MSM to find duplicate products\n",
    "# **---------------------------------------------------\n",
    "# ** [Ref Paper] Van Bezu, et al. (2015, April). Multi-component similarity method for web product duplicate detection.\n",
    "# =====================================================================================================================\n",
    "def calculate_qgram_similarity(str1, str2, q=3):\n",
    "    def qgrams(s, length):\n",
    "        return {s[i:i+length] for i in range(len(s)-length+1)} if len(s) >= length else {s}\n",
    "    \n",
    "    qg1 = qgrams(str1.lower(), q)\n",
    "    qg2 = qgrams(str2.lower(), q)\n",
    "    \n",
    "    intersect = qg1.intersection(qg2)\n",
    "    union = qg1.union(qg2)\n",
    "    return len(intersect) / len(union) if len(union) > 0 else 0.0\n",
    "\n",
    "def is_same_shop(p1,p2, df):\n",
    "    return df.at[p1, 'shop'] == df.at[p2, 'shop']\n",
    "    \n",
    "def is_different_brand(p1,p2,df):\n",
    "    return df.at[p1, 'brand'] != df.at[p2, 'brand']\n",
    "\n",
    "def extract_model_words_features(value):\n",
    "    pattern = re.compile(r'[a-zA-Z0-9]*(([0-9]+[^0-9, ]+)|([^0-9, ]+[0-9]+))[a-zA-Z0-9]*')\n",
    "    matches = re.findall(pattern, value.lower())\n",
    "    \n",
    "    mw = [word for word in matches if any(char.isdigit() for char in word) and any(char.isalpha() for char in word)]\n",
    "    unique_mw = list(dict.fromkeys(mw))\n",
    "    \n",
    "    return unique_mw\n",
    "\n",
    "def extract_mw_features(features):\n",
    "    mw = set()\n",
    "    for val in features.values():\n",
    "        mw.update(extract_model_words_features(val))\n",
    "    return mw\n",
    "    \n",
    "    \n",
    "def calculate_mwPerc(C,D):\n",
    "    if len(C) == 0 or len(D)==0:\n",
    "        return 0.0\n",
    "    intersect = C.intersection(D)\n",
    "    union = C.union(D)\n",
    "    return len(inter)/len(union) if len(uni)>0 else 0.0\n",
    "    \n",
    "def calculate_titleSim(tmw1, tmw2):\n",
    "    if len(tmw1) == 0 or len(tmw2) == 0:\n",
    "        return 0.0\n",
    "    intersection = tmw1.intersection(tmw2)\n",
    "    union = tmw1.union(tmw2)\n",
    "    return len(intersection) / len(union) if union else 0.0\n",
    "\n",
    "def build_dissimilarity_matrix_MSM(df, candidate_pairs, alpha=0.602, gamma=0.756, mu=0.650):\n",
    "    # Initialise dissimilarity matrix \n",
    "    N = len(df)\n",
    "    inf_distance = 500\n",
    "    diss_matrix = np.ones((N,N)) * inf_distance\n",
    "    np.fill_diagonal(diss_matrix, 0)\n",
    "\n",
    "    for (pi,pj) in candidate_pairs:\n",
    "        # Checks beforehand\n",
    "        if pi == pj:\n",
    "            continue\n",
    "        if is_same_shop(pi,pj,df) or is_different_brand(pi,pj,df):\n",
    "            diss_matrix[pi, pj] = inf_distance\n",
    "            diss_matrix[pj, pi] = inf_distance\n",
    "            continue\n",
    "\n",
    "        # Extract KVPs\n",
    "        KVPi = df.at[pi, 'features']\n",
    "        KVPj = df.at[pj, 'features']\n",
    "\n",
    "        # Initialise starting variables\n",
    "        sim = 0.0\n",
    "        avgSim = 0.0\n",
    "        m = 0\n",
    "        w = 0.0\n",
    "\n",
    "        nmki = dict(KVPi)\n",
    "        nmkj = dict(KVPj)\n",
    "\n",
    "        #(1) Calculate avgSim of KVP\n",
    "        for s_key, s_value in KVPi.items():\n",
    "            for t_key, t_value in KVPj.items():\n",
    "                keySim = calculate_qgram_similarity(s_key, t_key, q=3)\n",
    "                if keySim > gamma:\n",
    "                    valueSim = calculate_qgram_similarity(s_value, t_value, q=3)\n",
    "                    weight = keySim\n",
    "                    sim += weight * valueSim\n",
    "                    m += 1\n",
    "                    w += weight\n",
    "                    nmki.pop(s_key, None)\n",
    "                    nmkj.pop(t_key, None)\n",
    "        avgSim = (sim/m) if w>0 else 0.0\n",
    "        \n",
    "        #(2) Calculate mwPerc from non-matching keys\n",
    "        mw_i = extract_mw_features(nmki)\n",
    "        mw_j = extract_mw_features(nmkj)\n",
    "        mwPerc = calculate_mwPerc(mw_i,mw_j)\n",
    "\n",
    "        #(3) Calculate titleSim with TMWM\n",
    "        tmwi = set(df.at[pi, 'model words'])\n",
    "        tmwj = set(df.at[pj, 'model words'])\n",
    "        titleSim = calculate_titleSim(tmwi, tmwj)\n",
    "        \n",
    "        # Combine to obtain MSM \n",
    "        minFeatures = min(len(KVPi), len(KVPj))\n",
    "        if titleSim == -1:\n",
    "            theta1 = m/minFeatures\n",
    "            theta2 = 1 - theta1\n",
    "            hSim = theta1 * avgSim + theta2 * mwPerc\n",
    "        else:\n",
    "            theta1 = (1-mu) * (m/minFeatures)\n",
    "            theta2 = 1 - mu - theta1\n",
    "            hSim = theta1 * avgSim + theta2 * mwPerc + mu * titleSim \n",
    "\n",
    "        diss = 1 - hSim\n",
    "        diss_matrix[pi,pj] = diss\n",
    "        diss_matrix[pj,pi] = diss\n",
    "\n",
    "    return diss_matrix \n",
    "\n",
    "def apply_hClustering(diss_matrix, linkage='single', epsilon=0.52): \n",
    "    model = AgglomerativeClustering(distance_threshold=epsilon,\n",
    "                                    n_clusters=None,\n",
    "                                    linkage=linkage,\n",
    "                                    metric='precomputed')\n",
    "    model.fit(diss_matrix)\n",
    "\n",
    "    N = len(df)\n",
    "    inf_distance = 500\n",
    "    n_comparisons = np.zeros((N, N))\n",
    "    for i, j in np.argwhere((diss_matrix > 0) & (diss_matrix < inf_distance)):\n",
    "        n_comparisons[min(i, j), max(i, j)] = 1\n",
    "    n_comparisons = n_comparisons.sum()\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23eb69db-6902-47aa-8c03-841e4f41b222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# 5: [Functions] Evaluation Metrics\n",
    "# =============================================================\n",
    "\n",
    "def true_duplicates_found(unique_clusters, df):\n",
    "    found_duplicates = 0\n",
    "    \n",
    "    for c in tqdm(unique_clusters, desc=\"Check true clusters\"):\n",
    "        members = np.where(clusters==c)[0]\n",
    "        #print(f\"members to check: {members}\")\n",
    "        for i, j in combinations(members, 2):\n",
    "            if df.at[i, 'modelId'] == df.at[j, 'modelId']:\n",
    "                found_duplicates += 1\n",
    "                \n",
    "    return found_duplicates\n",
    "\n",
    "def true_duplicates_found_CandidateDuplicates(candidates, df):\n",
    "    found_duplicates = 0\n",
    "    \n",
    "    for i,j in candidates:\n",
    "        if df.at[i, 'modelId'] == df.at[j, 'modelId']:\n",
    "            found_duplicates += 1\n",
    "                \n",
    "    return found_duplicates\n",
    "\n",
    "def compute_total_duplicates(df):\n",
    "    \"\"\"\n",
    "    Compute the total number of true duplicate pairs (Dn) in the dataset.\n",
    "    Two products are true duplicates if they share the same modelId.\n",
    "    \"\"\"\n",
    "    groups = df.groupby(\"modelId\")\n",
    "    Dn = 0\n",
    "    for _, group_indices in groups.groups.items():\n",
    "        size = len(group_indices)\n",
    "        if size > 1:\n",
    "            Dn += size * (size - 1) // 2\n",
    "    return Dn\n",
    "\n",
    "def count_true_duplicates_in_pairs(pairs, df):\n",
    "    \"\"\"\n",
    "    Count how many of the given pairs are true duplicates.\n",
    "    pairs: iterable of (i,j)\n",
    "    df: DataFrame with 'modelId' column\n",
    "    \"\"\"\n",
    "    found_duplicates = 0\n",
    "    for (i, j) in pairs:\n",
    "        if df.at[i, 'modelId'] == df.at[j, 'modelId']:\n",
    "            found_duplicates += 1\n",
    "    return found_duplicates\n",
    "\n",
    "def count_true_duplicates_in_clusters(clusters, df):\n",
    "    \"\"\"\n",
    "    Count how many duplicate pairs are found within the identified clusters.\n",
    "    clusters: array of cluster labels for each product\n",
    "    df: DataFrame with 'modelId'\n",
    "    \"\"\"\n",
    "    found_duplicates = 0\n",
    "    unique_clusters = np.unique(clusters)\n",
    "    for c in unique_clusters:\n",
    "        members = np.where(clusters == c)[0]\n",
    "        # Count how many duplicate pairs among these members\n",
    "        for i, j in combinations(members, 2):\n",
    "            if df.at[i, 'modelId'] == df.at[j, 'modelId']:\n",
    "                found_duplicates += 1\n",
    "    return found_duplicates\n",
    "\n",
    "def compute_metrics_LSH(Df_LSH, Nc, Dn):\n",
    "    \"\"\"\n",
    "    Compute PQ, PC, and F1 for LSH:\n",
    "    PQ = Df_LSH / Nc\n",
    "    PC = Df_LSH / Dn\n",
    "    F1 = 2 * (PQ*PC)/(PQ+PC)\n",
    "    \"\"\"\n",
    "    if Nc == 0:\n",
    "        return 0.0, 0.0, 0.0  # no comparisons made\n",
    "    PQ = Df_LSH / Nc\n",
    "    if Dn == 0:\n",
    "        # If no duplicates in dataset, PC and F1 not meaningful\n",
    "        return PQ, 0.0, 0.0\n",
    "    PC = Df_LSH / Dn\n",
    "    if (PQ + PC) == 0:\n",
    "        F1 = 0.0\n",
    "    else:\n",
    "        F1 = 2 * PQ * PC / (PQ + PC)\n",
    "    return PQ, PC, F1\n",
    "\n",
    "def compute_metrics_MSM(Df_MSM, N_MSM, Dn):\n",
    "    \"\"\"\n",
    "    Compute precision, recall, and F1 for MSM:\n",
    "    precision = Df_MSM / N_MSM (duplicates flagged by MSM)\n",
    "    recall = Df_MSM / Dn\n",
    "    F1 = 2 * (precision*recall)/(precision+recall)\n",
    "    N_MSM = number of pairs flagged as duplicates by MSM\n",
    "            i.e. pairs from the clusters formed by MSM.\n",
    "\n",
    "    If you consider all pairs in the same cluster as \"flagged duplicates,\" then N_MSM is:\n",
    "      sum over all clusters: number_of_pairs_in_that_cluster\n",
    "    \"\"\"\n",
    "    if N_MSM == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    precision = Df_MSM / N_MSM\n",
    "    if Dn == 0:\n",
    "        # If no duplicates in dataset, recall and F1 not meaningful\n",
    "        return precision, 0.0, 0.0\n",
    "    recall = Df_MSM / Dn\n",
    "    if (precision + recall) == 0:\n",
    "        F1 = 0.0\n",
    "    else:\n",
    "        F1 = 2 * precision * recall / (precision + recall)\n",
    "    return precision, recall, F1\n",
    "\n",
    "def count_flagged_duplicates_by_MSM(clusters):\n",
    "    \"\"\"\n",
    "    Count how many pairs are flagged as duplicates by MSM clustering.\n",
    "    This means counting all pairs from each cluster, since MSM considers \n",
    "    all products in a cluster as duplicates.\n",
    "    \"\"\"\n",
    "    flagged_duplicates = 0\n",
    "    unique_clusters = np.unique(clusters)\n",
    "    for c in unique_clusters:\n",
    "        members = np.where(clusters == c)[0]\n",
    "        size = len(members)\n",
    "        if size > 1:\n",
    "            flagged_duplicates += size * (size - 1) // 2\n",
    "    return flagged_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bab402fe-76e6-4244-ac97-5c050f47fb91",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m epsilon_range \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.40\u001b[39m, \u001b[38;5;241m0.45\u001b[39m, \u001b[38;5;241m0.50\u001b[39m, \u001b[38;5;241m0.55\u001b[39m, \u001b[38;5;241m0.60\u001b[39m, \u001b[38;5;241m0.75\u001b[39m, \u001b[38;5;241m0.80\u001b[39m]\n\u001b[1;32m      7\u001b[0m linkage_methods \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msingle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomplete\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 9\u001b[0m df \u001b[38;5;241m=\u001b[39m load_data()\n\u001b[1;32m     10\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrand\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(extract_brand)\n\u001b[1;32m     11\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(clean_title)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_data' is not defined"
     ]
    }
   ],
   "source": [
    "# ====== RUN =================================================\n",
    "\n",
    "bootstrap = 10\n",
    "n_train = 1000\n",
    "t_range = [0.05, 0.10, 0.20, 0.3, 0.4,0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
    "epsilon_range = [0.40, 0.45, 0.50, 0.55, 0.60, 0.75, 0.80]\n",
    "linkage_methods = ['single', 'average', 'complete']\n",
    "\n",
    "df = load_data()\n",
    "df[\"brand\"] = df[\"title\"].apply(extract_brand)\n",
    "df[\"title\"] = df[\"title\"].apply(clean_title)\n",
    "df[\"model words\"] = df[\"title\"].apply(extract_model_words)\n",
    "\n",
    "N = df.shape[0]\n",
    "\n",
    "all_results=[]\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "for bootstrap in tqdm(range(n_bootstraps), desc=\"Bootstraps\"):\n",
    "    # Split in training and test set\n",
    "    train_indices = random.sample(range(N), n_train) \n",
    "    train_set = sorted(set(train_indices))\n",
    "    test_indices = [i for i in range(N) if i not in train_set]\n",
    "    \n",
    "    df_train = df.iloc[train_set].reset_index(drop=True).copy()\n",
    "    df_test = df.iloc[test_indices].reset_index(drop=True).copy()\n",
    "    \n",
    "    # Recalculate uniqueId for consistency after resetting the index\n",
    "    df_train['uniqueId'] = df_train.index\n",
    "    df_test['uniqueId'] = df_test.index\n",
    "\n",
    "    # Train metrics\n",
    "    N_train = len(df_train)\n",
    "    Dn_train = compute_total_duplicates(df_train)\n",
    "    total_pairs_train = N_train * (N_train - 1) / 2\n",
    "    \n",
    "    # Test metrics\n",
    "    N_test = len(df_test)\n",
    "    Dn_test = compute_total_duplicates(df_test)\n",
    "    total_pairs_test = N_test * (N_test - 1) / 2\n",
    "\n",
    "    # ==== TRAINING ====\n",
    "    # Bulild training set\n",
    "    binary_train = create_binary_vectors(df_train)\n",
    "    n_hashes_train = math.floor(df_train.shape[0] * 0.50)\n",
    "    signature_train = min_hashing(binary_train, n_hashes_train)\n",
    "\n",
    "    # Initialisiation\n",
    "    best_F1_train = {'single': -1, 'average': -1, 'complete': -1}\n",
    "    best_eps_train = {'single': None, 'average': None, 'complete': None}\n",
    "\n",
    "    for t in tqdm(t_range, desc=\"[Training] Iterating through t-values\"):\n",
    "\n",
    "        # Apply LSH and evaluate\n",
    "        candidates, b, r, t_actual = apply_lsh(signature_train, t)\n",
    "        Nc = len(candidates)\n",
    "        Df_LSH = count_true_duplicates_in_pairs(candidates, df_train)\n",
    "        PQ, PC, F1_LSH = compute_metrics_LSH(Df_LSH, Nc, Dn_train)\n",
    "        fraction_of_comparisons = Nc / total_pairs_train\n",
    "\n",
    "        # Build dissimilarity for MSM\n",
    "        dist_matrix = build_dissimilarity_matrix_MSM(df_train, candidates, alpha=0.602, gamma=0.756, mu=0.650)\n",
    "        \n",
    "        for eps in epsilon_range:\n",
    "\n",
    "            for linkage in linkage_methods: \n",
    "                # Apply MSM clustering and evaluate\n",
    "                model = apply_hClustering(dist_matrix, linkage=linkage, epsilon=eps)\n",
    "                cluster_labels = model.labels_\n",
    "                \n",
    "                Df_MSM = count_true_duplicates_in_clusters(cluster_labels, df_train)\n",
    "                N_MSM = count_flagged_duplicates_by_MSM(cluster_labels)\n",
    "                precision, recall, F1_MSM = compute_metrics_MSM(Df_MSM, N_MSM, Dn_train)\n",
    "\n",
    "                # Update best_epsilon based on F1 of MSM\n",
    "                if F1_MSM > best_F1_train[linkage]:\n",
    "                    best_F1_train[linkage] = F1_MSM\n",
    "                    best_eps_train[linkage] = eps\n",
    "\n",
    "                # Store results \n",
    "                all_results.append({\n",
    "                    'bootstrap': bootstrap,\n",
    "                    'bootstrap_sort': 'training',\n",
    "                    'b': b,\n",
    "                    'r': r,\n",
    "                    't': t_actual,\n",
    "                    'epsilon': eps,\n",
    "                    'linkage_method': linkage,\n",
    "                    'Nc': Nc,\n",
    "                    'Dn': Dn_train,\n",
    "                    'Df_LSH': Df_LSH,\n",
    "                    'PQ_LSH': PQ,\n",
    "                    'PC_LSH': PC,\n",
    "                    'F1_LSH': F1_LSH,\n",
    "                    'fraction_of_comparisons': fraction_of_comparisons,\n",
    "                    'Df_MSM': Df_MSM,\n",
    "                    'N_MSM': N_MSM,\n",
    "                    'precision_MSM': precision,\n",
    "                    'recall_MSM': recall,\n",
    "                    'F1_MSM': F1_MSM\n",
    "                })\n",
    "\n",
    "    \n",
    "    # ==== TESTING ====\n",
    "    # Bulild testing set \n",
    "    binary_test = create_binary_vectors(df_test)\n",
    "    n_hashes_test = math.floor(df_test.shape[0] * 0.5)\n",
    "    signature_test = min_hashing(binary_test, n_hashes_test)\n",
    "    \n",
    "    for t in tqdm(t_range, desc=\"[Testing] Iterating through t-values\"):\n",
    "        candidates, b, r, t_actual = apply_lsh(signature_test, t)\n",
    "        Nc = len(candidates)\n",
    "        Df_LSH = count_true_duplicates_in_pairs(candidates, df_test)\n",
    "        PQ, PC, F1_LSH = compute_metrics_LSH(Df_LSH, Nc, Dn_test)\n",
    "        fraction_of_comparisons = Nc / total_pairs_test\n",
    "    \n",
    "        # Build dissimilarity matrix MSM\n",
    "        dist_matrix = build_dissimilarity_matrix_MSM(df_test, candidates, alpha=0.602, gamma=0.756, mu=0.650)\n",
    "            \n",
    "        for linkage in linkage_methods: \n",
    "            best_epsilon = best_eps_train[linkage]\n",
    "            if best_epsilon is not None:\n",
    "                model = apply_hClustering(dist_matrix, linkage=linkage, epsilon=best_epsilon)\n",
    "                cluster_labels = model.labels_\n",
    "\n",
    "                Df_MSM = count_true_duplicates_in_clusters(cluster_labels, df_test)\n",
    "                N_MSM = count_flagged_duplicates_by_MSM(cluster_labels)\n",
    "                precision, recall, F1_MSM = compute_metrics_MSM(Df_MSM, N_MSM, Dn_test)\n",
    "\n",
    "                # Append test results\n",
    "                all_results.append({\n",
    "                    'bootstrap': bootstrap,\n",
    "                    'bootstrap_sort': 'test',\n",
    "                    'b': b,\n",
    "                    'r': r,\n",
    "                    't': t_actual,\n",
    "                    'epsilon': best_epsilon,\n",
    "                    'linkage_method': linkage,\n",
    "                    'Nc': Nc,\n",
    "                    'Dn': Dn_test,\n",
    "                    'Df_LSH': Df_LSH,\n",
    "                    'PQ_LSH': PQ,\n",
    "                    'PC_LSH': PC,\n",
    "                    'F1_LSH': F1_LSH,\n",
    "                    'fraction_of_comparisons': fraction_of_comparisons,\n",
    "                    'Df_MSM': Df_MSM,\n",
    "                    'N_MSM': N_MSM,\n",
    "                    'precision_MSM': precision,\n",
    "                    'recall_MSM': recall,\n",
    "                    'F1_MSM': F1_MSM\n",
    "                })\n",
    "                \n",
    "df_results = pd.DataFrame(all_results)\n",
    "df_results.to_excel(\"evaluation_results_bootstrap.xlsx\", index=False)\n",
    "print(\"Results saved to evaluation_results_bootstrap.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
